{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required files"
      ],
      "metadata": {
        "id": "_XOFoGPt0HP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlB78PIzZJnf"
      },
      "outputs": [],
      "source": [
        "%pip install sacremoses==0.0.53\n",
        "%pip install datasets\n",
        "%pip install transformers\n",
        "%pip install torch torchvision torchaudio\n",
        "%pip install datasets\n",
        "%pip install nltk\n",
        "%pip install rouge-score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing required files"
      ],
      "metadata": {
        "id": "Xx12m0bd0P4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from google.colab import drive\n",
        "from datasets import load_metric\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "svVTXl_wkGgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "B9xiQJRe0UhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xu3ELQQJcT1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the path to the CSV file"
      ],
      "metadata": {
        "id": "yUWjC4ca1nxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = '/content/drive/MyDrive/sampletrain.csv'"
      ],
      "metadata": {
        "id": "GKvkFnqpcyvz"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset from CSV"
      ],
      "metadata": {
        "id": "9PjAr9ML1rAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_dataset = load_dataset(\n",
        "    'csv',\n",
        "    data_files=path_to_file,\n",
        "    column_names=[\"document\", \"summary\"],\n",
        "    cache_dir='/Documents/Huggin_Face/data'\n",
        ")"
      ],
      "metadata": {
        "id": "X1NnenLK2CQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select a sample of the dataset"
      ],
      "metadata": {
        "id": "30NAXy212CC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_sample = xsum_dataset[\"train\"].select(range(1,11))"
      ],
      "metadata": {
        "id": "BZxI41IMaZnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display the sample data"
      ],
      "metadata": {
        "id": "POrx6cpU2KxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(xsum_sample.to_pandas())"
      ],
      "metadata": {
        "id": "JnqcFTWh2Lkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a summarization pipeline using t5-small"
      ],
      "metadata": {
        "id": "4iidN3d51tmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\n",
        "    task=\"summarization\",\n",
        "    model=\"t5-small\",\n",
        "    truncation=True,\n",
        "    model_kwargs={\"cache_dir\": '/Documents/Huggin_Face/'},\n",
        ")\n"
      ],
      "metadata": {
        "id": "_bXWM16JaeAm"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the tokenizer for the summarization model"
      ],
      "metadata": {
        "id": "8u-FDMyA2a1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "LtEcwzZy2aIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Input Text and Count Tokens in Example"
      ],
      "metadata": {
        "id": "I2QXc38r1wIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_index = 0\n",
        "\n",
        "input_text = xsum_sample[\"document\"][example_index]\n",
        "\n",
        "input_tokens = tokenizer.tokenize(input_text)\n",
        "\n",
        "num_tokens = len(input_tokens)\n",
        "\n",
        "print(\"Tokenized Input:\", input_tokens)\n",
        "\n",
        "print(\"Number of Tokens:\", num_tokens)\n"
      ],
      "metadata": {
        "id": "KfmbY6aiifsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a Summary using T5-Small Model with Specified Length Constraints"
      ],
      "metadata": {
        "id": "kBHK4WA61ygh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = xsum_sample[\"document\"][example_index]\n",
        "\n",
        "generated_summary = summarizer(input_text, max_length=50, min_length=20, do_sample=False)[0]['summary_text']"
      ],
      "metadata": {
        "id": "El2fFrdOpUF2"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate ROUGE score"
      ],
      "metadata": {
        "id": "rjqNZE0a3Rm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "reference_summary = xsum_sample[\"summary\"][example_index]\n",
        "\n",
        "scores = scorer.score(generated_summary, reference_summary)\n",
        "\n",
        "rouge1_scores = scores['rouge1'].fmeasure\n",
        "rougeL_scores = scores['rougeL'].fmeasure"
      ],
      "metadata": {
        "id": "g052ms4QrzyF"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenize reference and generated summaries, then calculate METEOR score"
      ],
      "metadata": {
        "id": "6C6Rolhb3csl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_tokens = [word_tokenize(sent) for sent in sent_tokenize(xsum_sample[\"summary\"][example_index])]\n",
        "generated_tokens = [word_tokenize(sent) for sent in sent_tokenize(generated_summary)]\n",
        "\n",
        "reference_tokens_flat = [token for sent in reference_tokens for token in sent]\n",
        "generated_tokens_flat = [token for sent in generated_tokens for token in sent]\n",
        "\n",
        "meteor = meteor_score([reference_tokens_flat], generated_tokens_flat)"
      ],
      "metadata": {
        "id": "1N0wBr6oah8S"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print Evaluation Results"
      ],
      "metadata": {
        "id": "yhjIzsil3lC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the generated summary, ROUGE-1 F1 score, ROUGE-L F1 score, and METEOR score."
      ],
      "metadata": {
        "id": "_JjU9UzD3k4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Summary:\", generated_summary)\n",
        "\n",
        "print(\"ROUGE-1 F1:\", rouge1_scores)\n",
        "\n",
        "print(\"ROUGE-L F1:\", rougeL_scores)\n",
        "\n",
        "print(\"METEOR Score:\", meteor)"
      ],
      "metadata": {
        "id": "GJOPXHT7uQ4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "49ntyj3X4CoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In brief, the provided code represents a sequence-to-sequence (s2s) model with an attention mechanism. It leverages the t5-small model to generate summaries for input documents. Evaluation metrics, including ROUGE and METEOR scores, help us to calculate the quality of the generated summaries against benchmark summaries. This approach is a common technique in natural language processing tasks such as text summarization."
      ],
      "metadata": {
        "id": "c-OZhB334FOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# By: Aditya Acharya"
      ],
      "metadata": {
        "id": "TYC3njTX36Fj"
      }
    }
  ]
}